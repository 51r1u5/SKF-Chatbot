{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 Questions of training data\n"
     ]
    }
   ],
   "source": [
    "# use natural language toolkit\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "# word stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "training_data=[]\n",
    "#with open(\"new.txt\", \"r\") as read_file:\n",
    " #   data = read_file.readlines()\n",
    "#for i in data:\n",
    " #    training_data.append(i)\n",
    "#training_data=training_data.split(\",\")\n",
    "#print(training_data[1])\n",
    "\n",
    "\n",
    "\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"What is Filename injection Path traversel ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"What does Filename injection Path traversel mean ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Tell me something about Filename injection Path traversel ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Filename injection Path traversel\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Explain Filename injection Path traversel ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Elaborate Filename injection Path traversel ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Can you tell me about Filename injection Path traversel ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"What do you know about Filename injection Path traversel ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"What can you tell me about Filename injection Path traversel ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"I want to know about XSS Filename injection Path traversel\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Do you have information about Filename injection Path traversel ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"What is xss injection ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"What does xss injection mean ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Tell me something about xss injection ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"xss injection\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Explain xss injection ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Elaborate xss injection ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Can you tell me about xss injection ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"What do you know about xss injection ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"What can you tell me about xss injection ?\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"I want to know about XSS xss injection\"})\n",
    "training_data.append({\"Class\": \"Description\",\"Question\": \"Do you have information about xss injection ?\"})\n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to solve Filename injection Path traversel ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to resolve Filename injection Path traversel ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to mitigate Filename injection Path traversel ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to stop Filename injection Path traversel ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to defend Filename injection Path traversel ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to get secured against Filename injection Path traversel ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to solve xss injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to resolve xss injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to mitigate xss injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to stop xss injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to defend xss injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to get secured against xss injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to solve Command injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to resolve Command injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to mitigate Command injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to stop Command injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to defend Command injection ?\"}) \n",
    "training_data.append({\"Class\": \"Solution\",\"Question\": \"How to get secured against Command injection ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "training_data.append({\"Class\": \"Code\",\"Question\": \"How to get secured against not available item ?\"})\n",
    "\n",
    "\n",
    "print (\"%s Questions of training data\" % len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-46d135c5dc52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# tokenize each Question into words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# ignore a some things\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'s\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# capture unique stemmed words in the training corpus\n",
    "corpus_words = {}\n",
    "class_words = {}\n",
    "# turn a list into a set (of unique items) and then a list again (this removes duplicates)\n",
    "classes = list(set([a['Class'] for a in training_data]))\n",
    "for c in classes:\n",
    "    # prepare a list of words within each class\n",
    "    class_words[c] = []\n",
    "\n",
    "# loop through each Question in our training data\n",
    "for data in training_data:\n",
    "    # tokenize each Question into words\n",
    "    for word in nltk.word_tokenize(data['Question']):\n",
    "        # ignore a some things\n",
    "        if word not in [\"?\", \"'s\"]:\n",
    "            # stem and lowercase each word\n",
    "            stemmed_word = stemmer.stem(word.lower())\n",
    "            # have we not seen this word already?\n",
    "            if stemmed_word not in corpus_words:\n",
    "                corpus_words[stemmed_word] = 1\n",
    "            else:\n",
    "                corpus_words[stemmed_word] += 1\n",
    "\n",
    "            # add the word to our words in class list\n",
    "            class_words[data['Class']].extend([stemmed_word])\n",
    "\n",
    "# we now have each stemmed word and the number of occurances of the word in our training corpus (the word's commonality)\n",
    "#print (\"Corpus words and counts: %s \\n\" % corpus_words)\n",
    "# also we have all words in each class\n",
    "#print (\"Class words: %s\" % class_words)\n",
    "# calculate a score for a given class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_score(Question, class_name, show_details=True):\n",
    "    score = 0\n",
    "    # tokenize each word in our new Question\n",
    "    for word in nltk.word_tokenize(Question):\n",
    "        # check to see if the stem of the word is in any of our classes\n",
    "        if stemmer.stem(word.lower()) in class_words[class_name]:\n",
    "            # treat each word with same weight\n",
    "            score += 1\n",
    "            \n",
    "            if show_details:\n",
    "                print (\"   match: %s\" % stemmer.stem(word.lower() ))\n",
    "    return score\n",
    "\n",
    "# we can now calculate a score for a new Question\n",
    "Question = input(\"Enter question\")\n",
    "\n",
    "# now we can find the class with the highest score\n",
    "for c in class_words.keys():\n",
    "    print (\"Class: %s  Score: %s \\n\" % (c, calculate_class_score(Question, c)))\n",
    "\n",
    "    # calculate a score for a given class taking into account word commonality\n",
    "def calculate_class_score(Question, class_name, show_details=True):\n",
    "    score = 0\n",
    "    # tokenize each word in our new Question\n",
    "    for word in nltk.word_tokenize(Question):\n",
    "        # check to see if the stem of the word is in any of our classes\n",
    "        if stemmer.stem(word.lower()) in class_words[class_name]:\n",
    "            # treat each word with relative weight\n",
    "            score += (1 / corpus_words[stemmer.stem(word.lower())])\n",
    "\n",
    "            if show_details:\n",
    "                print (\"   match: %s (%s)\" % (stemmer.stem(word.lower()), 1 / corpus_words[stemmer.stem(word.lower())]))\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the class with highest score for Question\n",
    "def classify(Question):\n",
    "    high_class = None\n",
    "    high_score = 0\n",
    "    # loop through our classes\n",
    "    for c in class_words.keys():\n",
    "        # calculate score of Question for each class\n",
    "        score = calculate_class_score_commonality(Question, c, show_details=False)\n",
    "        # keep track of highest score\n",
    "        if score > high_score:\n",
    "            high_class = c\n",
    "            high_score = score\n",
    "\n",
    "    return high_class, high_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
